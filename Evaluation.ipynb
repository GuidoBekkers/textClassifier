{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"dialog_acts.dat\", header=None, names=['data'])\n",
    "df['act'] = df['data'].str.split(' ').str[0]\n",
    "df['sentence'] = df['data'].str.split(' ').str[1:]\n",
    "df['sentence'] = df['sentence'].str.join(' ')\n",
    "df.drop(labels='data', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inform      39.841575\n",
       "request     25.465668\n",
       "thankyou    12.779891\n",
       "reqalts      6.850712\n",
       "null         6.321321\n",
       "affirm       4.533156\n",
       "negate       1.705815\n",
       "bye          1.043096\n",
       "confirm      0.674483\n",
       "hello        0.364692\n",
       "repeat       0.129407\n",
       "ack          0.109800\n",
       "deny         0.105878\n",
       "restart      0.054900\n",
       "reqmore      0.019607\n",
       "Name: act, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.act.value_counts()/25501 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraptions = {'doesnt': \"does not\",\n",
    "               'im': 'i am', \n",
    "               'dont': 'do not',\n",
    "               'id': 'i would'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'] = df['sentence'].replace(contraptions, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_length'] = df.sentence.str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25501.000000\n",
       "mean         3.841026\n",
       "std          3.067537\n",
       "min          1.000000\n",
       "25%          2.000000\n",
       "50%          3.000000\n",
       "75%          5.000000\n",
       "max         24.000000\n",
       "Name: sentence_length, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the machine learning and baseline systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples in train set: 22950\n",
      "Samples in test set: 2551\n"
     ]
    }
   ],
   "source": [
    "x = df['sentence']\n",
    "y = df['act']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10)\n",
    "print(f'\\nSamples in train set: {len(x_train)}')\n",
    "print(f'Samples in test set: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the majority class classifier on the test samples:\n",
      "accuracy: 0.3908271266170129\n",
      "precision: 0.03006362512438561\n",
      "recall: 0.07692307692307693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\mair\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the majority class classifier on the test samples:')\n",
    "y_pred = len(y_test) * ['inform']\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"recall: {recall_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the rulebased classifier on the test samples\n",
      "accuracy: 0.3214425715405723\n",
      "precision: 0.5241955394421667\n",
      "recall: 0.5355638912942366\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the rulebased classifier on the test samples')\n",
    "# dictionary with act as key and corresponding words as values\n",
    "rules = {'ack': ['okay', 'okay um', 'alright'], 'affirm': ['yes right', 'right', 'yes'],\n",
    "         'bye': ['see you', 'good bye', 'bye'], 'confirm': ['is it'], 'deny': ['i dont want'],\n",
    "         'hello': ['hi', 'hello'],\n",
    "         'inform': ['looking for'], 'negate': ['no'], 'repeat': ['can you repeat that', 'what did you say'],\n",
    "         'reqalts': ['how about'], 'reqmore': ['more'], 'request': ['what is', 'where'], 'restart': ['start over'],\n",
    "         'thankyou': ['thank you', 'thanks']}\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for x in x_test:\n",
    "    acts = []\n",
    "    for k, v in rules.items():\n",
    "        if any(keywords in x.lower() for keywords in v):\n",
    "            acts.append(k)\n",
    "    if not acts:\n",
    "        y_pred.append('null')\n",
    "    else:\n",
    "        y_pred.append(random.choice(acts))\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"recall: {recall_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentences = 0.80\n",
    "min_sentences = 1\n",
    "BOW_vect = CountVectorizer(max_df=max_sentences, min_df=1, lowercase=True, strip_accents='ascii')\n",
    "x_train_counts = BOW_vect.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparameters for Decision tree using grid search and cross validation.\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:   10.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the best hyperparameters on the test set\n",
      "accuracy: 0.9572716581732654\n",
      "precision: 0.7429236203993087\n",
      "recall: 0.7052122728339791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\mair\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Finding best hyperparameters for Decision tree using grid search and cross validation.')\n",
    "params = {'max_depth': list(range(1,25)),\n",
    "          'min_samples_split': [10, 50,100]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(), params, verbose=1, cv=3, n_jobs=-1)\n",
    "grid_search_cv.fit(x_train_counts, y_train)\n",
    "\n",
    "x_test_counts = BOW_vect.transform(x_test)\n",
    "y_pred = grid_search_cv.predict(x_test_counts)\n",
    "print('Evaluation of the best hyperparameters on the test set')\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"recall: {recall_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best parameters for SVM using grid search and cross validation.\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 out of 192 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the best hyperparameters on the test set\n",
      "accuracy: 0.9811838494707957\n",
      "precision: 0.8666384216680814\n",
      "recall: 0.8857753728770729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\mair\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Finding best parameters for SVM using grid search and cross validation.')\n",
    "\n",
    "params = params = {\n",
    "    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"penalty\" : [\"l2\", \"l1\", \"none\", \"elasticnet\"],\n",
    "}\n",
    "\n",
    "grid_search_cv = GridSearchCV(SGDClassifier(), params, verbose=1, cv=3, n_jobs=-1)\n",
    "grid_search_cv.fit(x_train_counts, y_train)\n",
    "\n",
    "x_test_counts = BOW_vect.transform(x_test)\n",
    "y_pred = grid_search_cv.predict(x_test_counts)\n",
    "print('Evaluation of the best hyperparameters on the test set')\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"recall: {recall_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best parameters for MLP using grid search and cross validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\mair\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the best hyperparameters on the test set\n",
      "accuracy: 0.9854958839670718\n",
      "precision: 0.8878533860942674\n",
      "recall: 0.8656205758743264\n"
     ]
    }
   ],
   "source": [
    "print('Finding best parameters for MLP using grid search and cross validation.')\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "grid_search_cv = GridSearchCV(MLPClassifier(max_iter=100), params, n_jobs=-1, cv=3)\n",
    "grid_search_cv.fit(x_train_counts, y_train)\n",
    "\n",
    "x_test_counts = BOW_vect.transform(x_test)\n",
    "y_pred = grid_search_cv.predict(x_test_counts)\n",
    "print('Evaluation of the best hyperparameters on the test set')\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"recall: {recall_score(y_test, y_pred, average='macro')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
